{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5492f04-ee16-4114-93c0-21f26a81e89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f43c9f8-ad39-4bbc-92bb-6ae335a3c7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9fb6d8a-0407-4194-ba92-7c33836d7daf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distiluse-base-multilingual-cased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5307035-54f6-4acf-a99e-67d0c9e2c266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "a= pd.read_parquet('./dataset.parquet')\n",
    "a['track_id']= a['track_id'].astype('str')\n",
    "a['string']=a['track_nm']+' '+a['artist_nm_list']\n",
    "a= a.set_index(np.arange(len(a)))\n",
    "b = a.sample(frac=1).reset_index(drop=True)\n",
    "original = a['string'].values\n",
    "similar = b['string'].values\n",
    "new_df=pd.DataFrame({'sentence1':original, 'sentence2':similar,'label':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c280add4-f6ab-4f8c-87f3-7bd2f3b0fc3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 359.03 GiB (GPU 0; 79.10 GiB total capacity; 2.88 GiB already allocated; 3.53 GiB free; 2.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m embeddings_1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence1\u001b[39m\u001b[38;5;124m'\u001b[39m], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m embeddings_2 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode(new_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentence2\u001b[39m\u001b[38;5;124m'\u001b[39m], convert_to_tensor\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 3\u001b[0m cosine_scores \u001b[38;5;241m=\u001b[39m \u001b[43mutil\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpytorch_cos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddings_2\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/kobert/lib/python3.8/site-packages/sentence_transformers/util.py:28\u001b[0m, in \u001b[0;36mpytorch_cos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpytorch_cos_sim\u001b[39m(a: Tensor, b: Tensor):\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    Computes the cosine similarity cos_sim(a[i], b[j]) for all i and j.\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;124;03m    :return: Matrix with res[i][j]  = cos_sim(a[i], b[j])\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcos_sim\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/envs/kobert/lib/python3.8/site-packages/sentence_transformers/util.py:49\u001b[0m, in \u001b[0;36mcos_sim\u001b[0;34m(a, b)\u001b[0m\n\u001b[1;32m     47\u001b[0m a_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(a, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m b_norm \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(b, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_norm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 359.03 GiB (GPU 0; 79.10 GiB total capacity; 2.88 GiB already allocated; 3.53 GiB free; 2.91 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "embeddings_1 = model.encode(new_df['sentence1'], convert_to_tensor=True)\n",
    "embeddings_2 = model.encode(new_df['sentence2'], convert_to_tensor=True)\n",
    "cosine_scores = util.pytorch_cos_sim(embeddings_1, embeddings_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74400f1b-20d2-4469-919c-a190aed9a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#Mean Pooling - Take attention mask into account for correct averaging\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\")\n",
    "\n",
    "tokenizer.tokenize(\n",
    "  doc_list[0],\n",
    "  return_tensors='pt',\n",
    "  truncation=True,\n",
    "  max_length=256,\n",
    "  pad_to_max_length=True\n",
    ")[0:20]\n",
    "\n",
    "#Tokenize sentences\n",
    "encoded_input = tokenizer(doc_list, padding=True, truncation=True, max_length=122, return_tensors='pt')\n",
    "\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/bert-base-nli-mean-tokens\").to(\"cuda\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model_output = model(input_ids=encoded_input[\"input_ids\"].to(\"cuda\"))\n",
    "\n",
    "#Perform pooling. In this case, mean pooling\n",
    "sentence_embeddings = mean_pooling(model_output, encoded_input['attention_mask'].to(\"cuda\"))\n",
    "\n",
    "cosine_scores = util.pytorch_cos_sim(sentence_embeddings, sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd1e3ca-7ace-4edf-b94e-8e6a46550bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5fc03e-869c-477c-b0ee-76db24d2c73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = kor_list\n",
    "corpus_embedding = model.encode(corpus, convert_to_tensor = True)\n",
    "query = random.sample(kor_list,1)\n",
    "query_embedding = model.encode(query, convert_to_tensor = True)\n",
    "cos_score = util.pytorch_cos_sim(query_embedding, corpus_embedding)[0]\n",
    "cos_score = cos_score.cpu()\n",
    "top_results = np.argpartition(-cos_scores, range(top_k))[0:top_k]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kobert",
   "language": "python",
   "name": "kobert"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
