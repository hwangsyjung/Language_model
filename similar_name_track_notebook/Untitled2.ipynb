{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10c4527e-93c2-4036-8cc5-afca2f326150",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Identity, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "class SigmoidContrastiveLearning(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        layers = 1,\n",
    "        init_temp = 10,\n",
    "        init_bias = -10\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.temperatures = nn.Parameter(torch.ones(layers, 1, 1) * math.log(init_temp))\n",
    "        self.bias = nn.Parameter(torch.ones(layers, 1, 1) * init_bias)\n",
    "\n",
    "    def device(self):\n",
    "        return next(self.parameters()).device\n",
    "\n",
    "    def forward(self, sims):\n",
    "        device = sims.device\n",
    "        if sims.ndim == 2:\n",
    "            sims = rearrange(sims, 'i j -> 1 i j')\n",
    "\n",
    "        n = sims.shape[-1] \n",
    "        sims = sims * self.temperatures.exp() + self.bias \n",
    "        labels = 2 * rearrange(torch.eye(n).to(device), 'i j -> 1 i j') - torch.ones_like(sims)\n",
    "\n",
    "        return -F.logsigmoid(labels * sims).sum() / n\n",
    "\n",
    "\n",
    "class NT_Xent(nn.Module):\n",
    "    def __init__(self, temperature: float=.5):\n",
    "        super(NT_Xent, self).__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * temperature)\n",
    "        self.criterion = nn.CrossEntropyLoss(reduction=\"sum\")\n",
    "        self.similarity_f = nn.CosineSimilarity(dim=2)\n",
    "        \n",
    "    def mask_correlated_samples(self, batch_size):\n",
    "        N = 2 * batch_size\n",
    "        mask = torch.ones((N, N), dtype=bool)\n",
    "        mask = mask.fill_diagonal_(0)\n",
    "        for i in range(batch_size):\n",
    "            mask[i, batch_size + i] = 0\n",
    "            mask[batch_size + i, i] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        batch_size = z_i.shape[0]\n",
    "        \n",
    "        N = 2 * batch_size\n",
    "        mask = self.mask_correlated_samples(batch_size)\n",
    "        z = torch.cat((z_i, z_j), dim=0)\n",
    "        \n",
    "        sim = self.similarity_f(z.unsqueeze(1), z.unsqueeze(0))\n",
    "        sim /= self.temperature\n",
    "        \n",
    "        sim_i_j = torch.diag(sim, batch_size) # 우상 삼각 행렬\n",
    "        sim_j_i = torch.diag(sim, -batch_size) # 좌하 삼각 행렬\n",
    "\n",
    "        positive_samples = torch.cat((sim_i_j, sim_j_i), dim=0).reshape(N,1)\n",
    "        negative_samples = sim[mask].reshape(N,-1)\n",
    "\n",
    "        labels = torch.zeros(N).to(positive_samples.device).long()\n",
    "        logits = torch.cat((positive_samples, negative_samples), dim=1)\n",
    "        loss = self.criterion(logits, labels) / N\n",
    "        return loss, labels, logits\n",
    "    \n",
    "class MuLaN(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "\n",
    "        dim_latent=128,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim_latent = dim_latent\n",
    "\n",
    "        self.audio_model = ASTModel(fstride=10, tstride=10)\n",
    "        self.text_model = AutoModel.from_pretrained(\"klue/bert-base\")\n",
    "\n",
    "        self.audio_to_latent = nn.Sequential(\n",
    "            nn.Linear(768, 768, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(768, dim_latent, bias=False)\n",
    "        )\n",
    "        \n",
    "        self.text_to_latent = nn.Sequential(\n",
    "            nn.Linear(768, 768, bias=False),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(768, dim_latent, bias=False)\n",
    "        )\n",
    "\n",
    "    def get_audio_latents(self, wavs):\n",
    "        audio_h = self.audio_model(wavs)\n",
    "        audio_latents = self.audio_to_latent(audio_h)\n",
    "        return audio_latents\n",
    "\n",
    "    def get_text_latents(self, input_ids, attention_mask):\n",
    "        text_h = self.text_model(input_ids.squeeze(1), attention_mask=attention_mask.squeeze(1))[0][:, 0, :]\n",
    "        text_letents = self.text_to_latent(text_h)\n",
    "        return text_letents\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        wavs=None,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        return_latents=False,\n",
    "        return_similarities=False,\n",
    "        return_pairwise_similarities=False,\n",
    "    ):\n",
    "        \n",
    "        audio_latents = self.get_audio_latents(wavs)\n",
    "        text_latents = self.get_text_latents(input_ids, attention_mask)\n",
    "\n",
    "        return audio_latents, text_latents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bb5ff77-adb0-4739-a12c-747886185649",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import os\n",
    "import random\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/bert-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "932f7617-1a40-45d1-9f9d-c283d1b41ba6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class DatasetBase(Dataset):\n",
    "    def __init__(self, batch_size: int, data_type: str):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        sf_path = '/data/mulan_text_dataset/shortform_total.parquet.gzip'\n",
    "        lf_path = '/data/mulan_text_dataset/longform_total.parquet.gzip'\n",
    "        pl_path = '/data/mulan_text_dataset/playlist_total.parquet.gzip'\n",
    "\n",
    "        self.shortform_df = pd.read_parquet(sf_path)\n",
    "        self.shortform_rows = self.shortform_df.shape[0]\n",
    "\n",
    "        self.longform_df = pd.read_parquet(lf_path)\n",
    "        self.longform_rows = self.longform_df.shape[0]\n",
    "\n",
    "        self.playlist_df = pd.read_parquet(pl_path)\n",
    "        self.playlist_rows = self.playlist_df.shape[0]\n",
    "\n",
    "        # The number of data should be  a multiple of the batch size\n",
    "        self.total_length = (\n",
    "            (self.shortform_rows + self.longform_rows + self.playlist_rows)\n",
    "            // self.batch_size\n",
    "            * self.batch_size\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.total_length\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        raise RuntimeError(\"abstract function\")\n",
    "\n",
    "    def get_fbank(\n",
    "        self,\n",
    "        path: str,\n",
    "        output_freq: int = 16000,\n",
    "        seconds: int = 30,\n",
    "        infer=None\n",
    "    ):\n",
    "        \"\"\"\n",
    "        음악 파일을 fbank 형태로 변환\n",
    "\n",
    "        Args:\n",
    "            filename (str): 음원 파일 경로.\n",
    "            output_freq (int, optional): fbank로 생성될 Hz. Defaults to 16000.\n",
    "            start_seconds (int, optional): fbank로 생성될 시작 시간. Defaults to 30.\n",
    "            end_seconds (int, optional): fbank로 생성될 종료 시간. Defaults to 60.\n",
    "        \"\"\"\n",
    "        self.infer = infer\n",
    "        waveform, orig_freq = torchaudio.load(path)\n",
    "        waveform  = torch.mean(waveform, dim=0, keepdim=True)\n",
    "        \n",
    "        resampler = T.Resample(\n",
    "            orig_freq=orig_freq, new_freq=output_freq, dtype=waveform.dtype\n",
    "        )\n",
    "        waveform = resampler(waveform)\n",
    "        \n",
    "        length = seconds * output_freq\n",
    "        if self.infer==True:\n",
    "            if waveform.shape[1] <= 30 * output_freq + length:\n",
    "                start = 0\n",
    "                end = waveform.shape[1]\n",
    "            else:  \n",
    "                start = 0\n",
    "                end = start+length\n",
    "        else:\n",
    "            if waveform.shape[1] <= length:\n",
    "                start = 0\n",
    "                end = waveform.shape[1]\n",
    "            else:\n",
    "                start = random.randint(0, waveform.shape[1] - length)\n",
    "                end = start + length\n",
    "\n",
    "        # print(f\"crop start: {start}, end: {end}\")\n",
    "        waveform = waveform[:, start:end]\n",
    "\n",
    "        # normalization\n",
    "        waveform = waveform - waveform.mean()\n",
    "\n",
    "        # wavefile -> mel filter back 만들어줌\n",
    "        fbank = torchaudio.compliance.kaldi.fbank(\n",
    "            waveform,\n",
    "            htk_compat=True,\n",
    "            sample_frequency=output_freq,\n",
    "            use_energy=False,\n",
    "            window_type=\"hamming\",\n",
    "            num_mel_bins=128,\n",
    "            dither=0.0,\n",
    "            frame_shift=10,\n",
    "        )\n",
    "        # torchaudio.compliance.kaldi.fbank를 거치면 웨이브파일 -> (frame개수, mel filter 개수)\n",
    "        # input wave의 maximum 길이를 1000으로 맞춤\n",
    "        target_length = 3000\n",
    "        n_frames = fbank.shape[0]\n",
    "        p = target_length - n_frames\n",
    "\n",
    "        # max 길이보다 크면 자르고, max길이보다 작으면 zero padding\n",
    "        if p > 0:\n",
    "            m = torch.nn.ZeroPad2d((0, 0, 0, p))\n",
    "            fbank = m(fbank)\n",
    "        elif p < 0:\n",
    "            fbank = fbank[0:target_length, :]\n",
    "\n",
    "        return fbank\n",
    "\n",
    "    def get_music_path(self, folder, music_id, ext):\n",
    "        music_path = (\n",
    "            f\"{folder}/{music_id}.aac\" if ext == None else f\"{folder}/{music_id}.{ext}\"\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(music_path):\n",
    "            music_path = f\"{folder}/{music_id}.m4a\"\n",
    "\n",
    "        return music_path\n",
    "\n",
    "class InferDataset(DatasetBase):\n",
    "    def __init__(self, batch_size: int):\n",
    "        super().__init__(batch_size=batch_size, data_type=\"total\")\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        try:\n",
    "            return self.getitem(idx)\n",
    "        except:\n",
    "            return self.getitem(idx - 1)\n",
    "\n",
    "    def getitem(self, idx):\n",
    "        caption = self.shortform_df[\"text\"][idx]\n",
    "        music_id = self.shortform_df[\"id\"][idx]\n",
    "\n",
    "        encoding = tokenizer(caption, return_tensors='pt', padding='max_length', truncation=True, max_length=512)\n",
    "        input_ids = encoding['input_ids']\n",
    "        attention_mask = encoding['attention_mask']\n",
    "        \n",
    "        music_path = self.get_music_path(folder=\"music\", music_id=music_id, ext=None)\n",
    "        fbank = self.get_fbank(music_path, infer=True)\n",
    "        # ast에서 input spectrogram norm 시킴\n",
    "        fbank = (fbank - (-4.2677393)) / (4.5689974 * 2)\n",
    "\n",
    "        return music_id, caption, input_ids, attention_mask, fbank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "805ab08e-cfa6-4ff6-86cc-cd337aff1219",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_path = '/data/mulan_output_v7/'\n",
    "    \n",
    "batch_size = 2\n",
    "mulan = MuLaN()\n",
    "infer_dataset = InferDataset(batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4dc5dd5a-dee0-4af4-810f-a51cb888ea09",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MuLaN:\n\tUnexpected key(s) in state_dict: \"text_model.embeddings.position_ids\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m pkg \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;28mstr\u001b[39m(path), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m mulan \u001b[38;5;241m=\u001b[39m MuLaN()        \n\u001b[0;32m---> 14\u001b[0m \u001b[43mmulan\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpkg\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m mulan \u001b[38;5;241m=\u001b[39m mulan\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/opt/conda/envs/iris/lib/python3.8/site-packages/torch/nn/modules/module.py:2041\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   2036\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2037\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2038\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(k) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2041\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2042\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2043\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MuLaN:\n\tUnexpected key(s) in state_dict: \"text_model.embeddings.position_ids\". "
     ]
    }
   ],
   "source": [
    "infer_loader = DataLoader(\n",
    "    dataset=infer_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    prefetch_factor=4,\n",
    ")\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "from pathlib import Path\n",
    "path = Path('/data/mulan_checkpoints/mulan.v6.490000.pt')\n",
    "pkg = torch.load(str(path), map_location='cpu')\n",
    "mulan = MuLaN()        \n",
    "mulan.load_state_dict(pkg[\"model\"])\n",
    "mulan = mulan.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86962edc-ac60-4118-9b95-4ddf89b8720c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_audio_path = os.path.join(output_path, \"audio_vector\")\n",
    "output_text_path = os.path.join(output_path, \"text_vector\")\n",
    "    \n",
    "Path(output_audio_path).mkdir(parents=True, exist_ok=True)\n",
    "Path(output_text_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "for idx, batch in enumerate(infer_loader):\n",
    "        v_audio_path = os.path.join(output_audio_path, f\"vector_audio_{str(idx)}.json\")\n",
    "        v_text_path = os.path.join(output_text_path, f\"vector_text_{str(idx)}.json\")\n",
    "        audio_features = {}\n",
    "        text_features = {}\n",
    "        music_id, caption = batch[0], batch[1]\n",
    "        input_ids, attention_mask, audio = batch[2], batch[3], batch[4]\n",
    "\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        audio = audio.to(device)\n",
    "\n",
    "        audio_embeds = mulan.get_audio_latents(audio)  # during training\n",
    "        text_embeds = mulan.get_text_latents(input_ids, attention_mask)  # during inference\n",
    "        audio_out = F.normalize(audio_embeds, p=2, dim=-1)\n",
    "        text_out = F.normalize(text_embeds, p=2, dim=-1)\n",
    "        for a,b,c,d in zip(music_id, caption, audio_out, text_out):\n",
    "            cbf_id = os.path.basename(str(int(a.numpy())))\n",
    "            audio_features[cbf_id] = c.detach().cpu().numpy().tolist()\n",
    "            text_features[b] = d.detach().cpu().numpy().tolist()\n",
    "        with open(v_audio_path, 'w') as outfile:\n",
    "            json.dump(audio_features, outfile)\n",
    "        with open(v_text_path, 'w') as outfile:\n",
    "            json.dump(text_features, outfile)\n",
    "        del audio_embeds\n",
    "        del text_embeds\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris",
   "language": "python",
   "name": "iris"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
